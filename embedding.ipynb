{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWOyc4mFnweZto2zJHLRF6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnandK-pm/DSA-C/blob/main/embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A demonstration of embedding layer using keras sequential API"
      ],
      "metadata": {
        "id": "FJQSUSYZhWe4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRMAwzTfObJO"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential   #uses sequential API\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "import numpy as np\n",
        "# define documents\n",
        "docs = ['Well done!',\n",
        " 'Good work',\n",
        " 'Great effort',\n",
        " 'nice work',\n",
        " 'Excellent!',\n",
        " 'Weak',\n",
        " 'Poor effort!',\n",
        " 'not good',\n",
        " 'poor work',\n",
        " 'Could have done better.']\n",
        "# define class labels\n",
        "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "one_hot() function that creates a hash of each word as an efficient integer encoding. We will estimate the vocabulary size of 50, which is much larger than needed to reduce the probability of collisions from the hash function."
      ],
      "metadata": {
        "id": "3qtrF6zrQO84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLPa6znCOnje",
        "outputId": "5524b83c-8fbc-4cc0-fecd-3ba27d28c15d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[44, 4], [35, 11], [22, 14], [13, 11], [45], [36], [45, 14], [5, 35], [45, 11], [30, 20, 4, 32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pad documents to a max length of 4 words\n",
        "# as each sequence is of different length ,this is done for uniformity\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhQ2tOqBPc3P",
        "outputId": "ba5b5762-cb08-4de0-e9ae-5d9f62e02c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[44  4  0  0]\n",
            " [35 11  0  0]\n",
            " [22 14  0  0]\n",
            " [13 11  0  0]\n",
            " [45  0  0  0]\n",
            " [36  0  0  0]\n",
            " [45 14  0  0]\n",
            " [ 5 35  0  0]\n",
            " [45 11  0  0]\n",
            " [30 20  4 32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to define the neural network. It consits of 3 layers\n",
        "1. Embedding layer\n",
        "2. Flatten layer\n",
        "3. Dense layer\n",
        "\n",
        "Embedding layer is fed with the padded docs. which contains 4*8 3D matrices.\n",
        "It is flattened to 1D vector in the flatten layer as dense layer only accepts 1D vectors.\n",
        "Dense layer which in this case only contains one neuron , as we only have to predict 1 or 0 (binary) produces the final output between 0 and 1.\n",
        "(0.7 means a probability of being 1 is 70%)\n"
      ],
      "metadata": {
        "id": "jB4hXT7Ffbyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35fIsSeOfW3L",
        "outputId": "1047c121-7082-480e-e50b-67d604bb1209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 8)              400       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 433 (1.69 KB)\n",
            "Trainable params: 433 (1.69 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 4, 8)              400       \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 433 (1.69 KB)\n",
            "Trainable params: 433 (1.69 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we have to train the model with the actual labels we have. For that we use the fit function on the model defined with giving the padded docs as input."
      ],
      "metadata": {
        "id": "G6p9s-Rcgox6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "#epochs-This parameter specifies the number of times the entire dataset\n",
        "        #will be passed forward and backward through the neural network\n",
        "        #during training.\n",
        "\n",
        "#verbose-whether to print output during training. 0-no , 1 or 2 -yes\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6V_J7ylgyFq",
        "outputId": "4d4ef6b3-a289-4d11-d79c-775873562b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 89.999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to analyze sentiment of a single word\n",
        "def analyze_sentiment(word):\n",
        "    # Preprocess the input word\n",
        "    encoded_word = [one_hot(word, vocab_size)]\n",
        "    padded_word = pad_sequences(encoded_word, maxlen=max_length, padding='post')\n",
        "    # Predict sentiment using the trained model\n",
        "    prediction = model.predict(padded_word)\n",
        "    # Return sentiment prediction (0 for negative, 1 for positive)\n",
        "    return 1 if prediction > 0.5 else 0\n",
        "input_word = input(\"Enter a word to analyze sentiment: \")\n",
        "sentiment = analyze_sentiment(input_word)\n",
        "if sentiment == 1:\n",
        "    print(\"Positive sentiment\")\n",
        "else:\n",
        "    print(\"Negative sentiment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISxNNfSjhxyU",
        "outputId": "db210e9d-5662-4006-de92-8419d62e1605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word to analyze sentiment: nice effort\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Positive sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PZ-P3kHIhTrQ"
      }
    }
  ]
}